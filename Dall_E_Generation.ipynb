{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "E0lST-nhvubF",
      "metadata": {
        "id": "E0lST-nhvubF"
      },
      "source": [
        "### Colab by mega b#6696\n",
        "\n",
        "##### [Original Models](https://github.com/robvanvolt/DALLE-models) & [Colab](https://github.com/afiaka87/dalle-pytorch-pretrained) simplified.\n",
        "\n",
        "---\n",
        "\n",
        "#### Join the [Dall-E PyTorch Discord server](https://discord.gg/VHqAXKF3p9) to help with recreating Dall-E!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "rS7ZTHTy3hM-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500,
          "referenced_widgets": [
            "e42c33fef9c948cf9657f4ba53000e4f",
            "42c516d1f9554aa480881d309dfbe8da",
            "1d36c9604ca242df8f7c4351e83a196e",
            "f1e44170012c4a5a8c4528ac8d9693dd",
            "1b338a8878e74c8e8a75666331231934",
            "e60cd7f49ee343e69433abf14c17d772",
            "ce482436024942e492651ec508bdb7f0",
            "caaab135490c4e1a9de134d18287d120",
            "a41fa6a659e84987a3fda99398ef5563",
            "f60e5268d4614f4fb959c98a9f031ff2",
            "45f8f979147c4a2d97db0310e34d9ec0"
          ]
        },
        "id": "rS7ZTHTy3hM-",
        "outputId": "407f28b2-90e4-49c9-db48-6baf56bf22e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done, move onto the next cell.\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **1** Install required components.\n",
        "from IPython.display import clear_output\n",
        "from google.colab.output import eval_js\n",
        "eval_js('google.colab.output.setIframeHeight(\"500\")')\n",
        "\n",
        "!nvidia-smi -L\n",
        "print(\"Installing...\")\n",
        "!pip -q install tqdm\n",
        "from tqdm.notebook import *\n",
        "with tqdm(total=10) as pbar:\n",
        "  !pip -q install keras==2.4.0\n",
        "  pbar.update(1)\n",
        "  !pip -q install git+https://github.com/afiaka87/CLIP.git\n",
        "  pbar.update(1)\n",
        "  !pip -q install taming-transformers\n",
        "  pbar.update(1)\n",
        "  !pip -q install dalle-pytorch==0.14.3\n",
        "  pbar.update(1)\n",
        "  !pip -q install tokenizers\n",
        "  pbar.update(1)\n",
        "  !pip -q install ftfy\n",
        "  pbar.update(1)\n",
        "  !pip -q install regex\n",
        "  pbar.update(1)\n",
        "  !pip -q install triton==0.4.2\n",
        "  pbar.update(1)\n",
        "  !git clone https://github.com/johnpaulbin/dalle-pytorch-pretrained.git\n",
        "  pbar.update(1)\n",
        "  !pip -q install wandb\n",
        "  pbar.update(1)\n",
        "%cd dalle-pytorch-pretrained\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(\"Done! We can now go to the next cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c6e0d9d2-b0a4-47e4-be51-8963aaa375bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "c6e0d9d2-b0a4-47e4-be51-8963aaa375bb",
        "outputId": "05dbb80d-d2fd-4fef-bd13-96bfa2c30a42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished, move onto the next cell.\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **2** Install required dependencies.\n",
        "\n",
        "eval_js('google.colab.output.setIframeHeight(\"250\")')\n",
        "\n",
        "#!wget --no-clobber https://www.dropbox.com/s/hl5hyzhyal3vfye/dalle_iconic_butterfly_149.pt\n",
        "%pip install tokenizers\n",
        "from tokenizers import Tokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = Tokenizer.from_file(\"/content/dalle-pytorch-pretrained/cc12m_tokenizer.json\")\n",
        "\n",
        "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
        "\n",
        "def tokenize(texts, context_length = 256, add_start = False, add_end = False, truncate_text = False):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "\n",
        "    sot_tokens = tokenizer.encode(\"<|startoftext|>\").ids if add_start else []\n",
        "    eot_tokens = tokenizer.encode(\"<|endoftext|>\").ids if add_end else []\n",
        "    all_tokens = [sot_tokens + tokenizer.encode(text).ids + eot_tokens for text in texts]\n",
        "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
        "\n",
        "    for i, tokens in enumerate(all_tokens):\n",
        "        if len(tokens) > context_length:\n",
        "            if truncate_text:\n",
        "                tokens = tokens[:context_length]\n",
        "            else:\n",
        "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
        "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
        "\n",
        "    return result\n",
        "\n",
        "!wget --no-clobber <dropbox_url>\n",
        "\n",
        "%pip install gpustat\n",
        "!wget \"https://github.com/lucidrains/DALLE-pytorch/archive/refs/tags/0.14.3.zip\" -O /content/\n",
        "!unzip /content/0.14.3.zip -d /content/dalle-pytorch-pretrained\n",
        "!mv /content/dalle-pytorch-pretrained/DALLE-pytorch-0.14.3 /content/dalle-pytorch-pretrained/DALLE-pytorch\n",
        "%cd ./DALLE-pytorch/\n",
        "!python3 setup.py install\n",
        "!sudo apt-get -y install llvm-9-dev cmake\n",
        "!git clone https://github.com/microsoft/DeepSpeed.git /tmp/Deepspeed\n",
        "%cd /tmp/Deepspeed\n",
        "!DS_BUILD_SPARSE_ATTN=1 ./install.sh -r\n",
        "\n",
        "!pip install deepspeed\n",
        "\n",
        "%cd /content/\n",
        "!apt-get install pv\n",
        "!apt-get install jq\n",
        "!wget https://raw.githubusercontent.com/tonikelope/megadown/master/megadown -O megadown.sh\n",
        "!chmod 755 megadown.sh\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(\"Finished, move onto the next cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2Pl8ovcl5T8h",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "2Pl8ovcl5T8h",
        "outputId": "a83460dc-b085-484a-9656-ff3ad6c6edf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "--2023-02-17 13:08:07--  https://github.com/johnpaulbin/DALLE-models/releases/download/model/16L_64HD_8H_512I_128T_cc12m_cc3m_3E.pt\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/379488067/2c1416ce-9ca2-4292-85c2-1f9f61ce642f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230217%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230217T130807Z&X-Amz-Expires=300&X-Amz-Signature=ac7b123f9e1765baee0ab6c9e9980c9b42f04921a3a0c47b9b5e07ac89219671&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=379488067&response-content-disposition=attachment%3B%20filename%3D16L_64HD_8H_512I_128T_cc12m_cc3m_3E.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-02-17 13:08:07--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/379488067/2c1416ce-9ca2-4292-85c2-1f9f61ce642f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230217%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230217T130807Z&X-Amz-Expires=300&X-Amz-Signature=ac7b123f9e1765baee0ab6c9e9980c9b42f04921a3a0c47b9b5e07ac89219671&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=379488067&response-content-disposition=attachment%3B%20filename%3D16L_64HD_8H_512I_128T_cc12m_cc3m_3E.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 835950622 (797M) [application/octet-stream]\n",
            "Saving to: ‘dalle_checkpoint.pt’\n",
            "\n",
            "dalle_checkpoint.pt  76%[==============>     ] 606.32M  25.7MB/s    eta 4s     "
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "\n",
        "import wandb\n",
        "\n",
        "\n",
        "eval_js('google.colab.output.setIframeHeight(\"500\")')\n",
        "\n",
        "#@markdown # **3** Choose the Dall-E Model.\n",
        "\n",
        "# Using https://github.com/robvanvolt/DALLE-models/tree/main/models/taming_transformer/64L_64HD_8H_756I_128T_cc12m_1E\n",
        "\n",
        "# Old model (Recommended): https://mega.nz/#!kShC2QjR!5BEPvrouy89XgRFo130hYdSLZu_hyz9s7oWUnhQsXb4\n",
        "\n",
        "# New model (More general): https://mega.nz/#!5PhBUCSD!kVzo_VFJde1kxPn3-cPpu2cN5dZwaBxFEO_o4hm9RSM\n",
        "\n",
        "chosen_model = \"3 Epoch 12M (by robvanvolt) (Highly recommended!)\" #@param [\"Old Model (by robvanvolt)\", \"May 20 model (More General Outputs)\", \"Old COCO model (by afiaka87)\", \"New model illustrations_imagenetvqgan (by afiaka87)\", \"2 Epoch 12M (by robvanvolt)\", \"3 Epoch 12M (by robvanvolt) (Highly recommended!)\"]\n",
        "\n",
        "if chosen_model == \"Old Model (by robvanvolt)\":\n",
        "  chosen_model = \"https://mega.nz/#!kShC2QjR!5BEPvrouy89XgRFo130hYdSLZu_hyz9s7oWUnhQsXb4\"\n",
        "elif chosen_model == \"May 20 model (More General Outputs)\":\n",
        "  chosen_model = \"https://mega.nz/#!5PhBUCSD!kVzo_VFJde1kxPn3-cPpu2cN5dZwaBxFEO_o4hm9RSM\"\n",
        "elif chosen_model == \"New model illustrations_imagenetvqgan (by afiaka87)\":\n",
        "  pass\n",
        "elif chosen_model == \"Old COCO model (by afiaka87)\":\n",
        "  chosen_model = \"https://www.dropbox.com/s/oper4enc0s0r738/vg_coco_oi_cc100k_latest.pt?dl=1\"\n",
        "elif chosen_model == \"2 Epoch 12M (by robvanvolt)\":\n",
        "  chosen_model = \"https://api.wandb.ai/files/robvanvolt/dalle_train_transformer/3bwiteds/dalle.pt\"\n",
        "elif chosen_model == \"3 Epoch 12M (by robvanvolt) (Highly recommended!)\":\n",
        "  chosen_model = \"https://github.com/johnpaulbin/DALLE-models/releases/download/model/16L_64HD_8H_512I_128T_cc12m_cc3m_3E.pt\"\n",
        "\n",
        "if \"https://mega.nz\" in chosen_model:\n",
        "  !/content/megadown.sh $chosen_model --o dalle_checkpoint.pt\n",
        "elif chosen_model == \"New model illustrations_imagenetvqgan (by afiaka87)\":\n",
        "  run = wandb.init()\n",
        "  artifact = run.use_artifact('dalle-pytorch-replicate/royalty_free_illustrations/trained-dalle:v7', type='model')\n",
        "  artifact_dir = artifact.download()\n",
        "else:\n",
        "  !wget \"$chosen_model\" -O dalle_checkpoint.pt\n",
        "\n",
        "clear_output()\n",
        "\n",
        "!mkdir -p ~/.cache/dalle;\n",
        "\n",
        "!wget https://www.dropbox.com/s/15mhdhy57y6qttf/vqgan.1024.model.ckpt;\n",
        "\n",
        "!wget https://www.dropbox.com/s/q8nayimg4skf0pl/vqgan.1024.config.yml;\n",
        "\n",
        "!wget https://www.dropbox.com/s/r4uukngelv2vhk3/variety.bpe?dl=1 -O variety.bpe\n",
        "\n",
        "!cp \"vqgan.1024.model.ckpt\" ~/.cache/dalle;\n",
        "!cp \"vqgan.1024.config.yml\" ~/.cache/dalle;\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(\"Finished downloading the selected model.\")\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "682c5804-5f97-469f-8cf1-1cc8356591b8",
      "metadata": {
        "id": "682c5804-5f97-469f-8cf1-1cc8356591b8"
      },
      "outputs": [],
      "source": [
        "#@markdown # **4** Try out the model.\n",
        "#@markdown #### Results will be saved in the outputs directory. Refresh (right click the folder -> refresh) if you dont see the result inside the folder.\n",
        "if chosen_model != \"New model illustrations_imagenetvqgan (by afiaka87)\":\n",
        "  checkpoint_path = \"/content/dalle_checkpoint.pt\"\n",
        "else:\n",
        "  checkpoint_path = \"/content/artifacts/trained-dalle:v7/dalle.pt\"\n",
        "\n",
        "text = \"the grand canyon with snow on it. snow located on the grand canyon. a snowy grand canyon.\" #@param {type:\"string\"}\n",
        "\n",
        "generate_16_images = False #@param {type:\"boolean\"}\n",
        "\n",
        "num_images = 8\n",
        "batch_size = 8\n",
        "\n",
        "if generate_16_images:\n",
        "  num_images = 16\n",
        "  batch_size = 16\n",
        "\n",
        "\n",
        "text_cleaned = text.replace(\" \", \"_\")\n",
        "_folder = f\"/content/outputs/{text_cleaned}/\"\n",
        "\n",
        "allow = [\"https://www.dropbox.com/s/oper4enc0s0r738/vg_coco_oi_cc100k_latest.pt?dl=1\", \"New model illustrations_imagenetvqgan (by afiaka87)\"]\n",
        "\n",
        "if chosen_model not in allow:\n",
        "  !python /content/dalle-pytorch-pretrained/DALLE-pytorch/generate.py --dalle_path=$checkpoint_path --taming --text=\"$text\" --num_images=$num_images --batch_size=$batch_size --outputs_dir=\"$_folder\"; wait;\n",
        "else:\n",
        "  !python /content/dalle-pytorch-pretrained/DALLE-pytorch/generate.py --dalle_path=$checkpoint_path --taming --text=\"$text\" --num_images=$num_images --batch_size=$batch_size --outputs_dir=\"$_folder\" --bpe_path variety.bpe; wait;\n",
        "\n",
        "#clear_output()\n",
        "\n",
        "print(\"Finished generating images, attempting to display results...\")\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "final = text_cleaned[:100]\n",
        "\n",
        "text_cleaned = text.replace(\" \", \"_\")\n",
        "output_dir = f\"/content/outputs/{text_cleaned}/{final}/\" \n",
        "images = []\n",
        "\n",
        "for img_path in glob.glob(f'{output_dir}*.jpg'):\n",
        "    images.append(mpimg.imread(img_path))\n",
        "\n",
        "plt.figure(figsize=(32,32))\n",
        "\n",
        "if generate_16_images:\n",
        "  plt.figure(figsize=(64,64))\n",
        "\n",
        "columns = 4\n",
        "for i, image in enumerate(images):\n",
        "    plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
        "    plt.imshow(image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F2xJouLI7aU2",
      "metadata": {
        "id": "F2xJouLI7aU2"
      },
      "source": [
        "# Optional cells\n",
        "### May break the session if used. If so, factory reset runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01aea99b-c91d-40e9-97cb-d23b462fbc44",
      "metadata": {
        "id": "01aea99b-c91d-40e9-97cb-d23b462fbc44"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "\n",
        "text_cleaned = text.replace(\" \", \"_\")\n",
        "output_dir = f\"/content/outputs/{text_cleaned}/\" #@param\n",
        "images = []\n",
        "for img_path in glob.glob(f'{output_dir}*.jpg'):\n",
        "    images.append(mpimg.imread(img_path))\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "columns = 5\n",
        "for i, image in enumerate(images):\n",
        "    plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
        "    plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "709d91bf-ec7d-43d7-ba3c-4305ebd4cf0f",
      "metadata": {
        "id": "709d91bf-ec7d-43d7-ba3c-4305ebd4cf0f"
      },
      "outputs": [],
      "source": [
        "%pip install \"git+https://github.com/openai/CLIP.git\"\n",
        "import clip\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab5300e3-d950-420f-add8-23aa5e005610",
      "metadata": {
        "id": "ab5300e3-d950-420f-add8-23aa5e005610"
      },
      "outputs": [],
      "source": [
        "\"\"\" Get rank by CLIP! \"\"\"\n",
        "image = F.interpolate(images, size=224)\n",
        "text = clip.tokenize([\"this colorful bird has a yellow breast , with a black crown and a black cheek patch.\"]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image)\n",
        "    text_features = model.encode_text(text)\n",
        "    \n",
        "    logits_per_image, logits_per_text = model(image, text)\n",
        "    probs = logits_per_text.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "print(\"Label probs:\", probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee4d809f-32af-42f8-ad7a-698792e501ce",
      "metadata": {
        "id": "ee4d809f-32af-42f8-ad7a-698792e501ce"
      },
      "outputs": [],
      "source": [
        "np_images = images.cpu().numpy()\n",
        "scores = probs[0]\n",
        "\n",
        "def show_reranking(images, scores, sort=True):\n",
        "    img_shape = images.shape\n",
        "    if sort:\n",
        "        scores_sort = scores.argsort()\n",
        "        scores = scores[scores_sort[::-1]]\n",
        "        images = images[scores_sort[::-1]]\n",
        "\n",
        "    rows = 4\n",
        "    cols = img_shape[0] // 4\n",
        "    img_idx = 0\n",
        "\n",
        "    for col in range(cols):\n",
        "        fig, axs = plt.subplots(1, rows, figsize=(20,20))\n",
        "        plt.subplots_adjust(wspace=0.01)\n",
        "        for row in range(rows):\n",
        "            tran_img = np.transpose(images[img_idx], (1,2,0))\n",
        "            axs[row].imshow(tran_img, interpolation='nearest')\n",
        "            axs[row].set_title(\"{}%\".format(np.around(scores[img_idx]*100, 5)))\n",
        "            axs[row].set_xticks([])\n",
        "            axs[row].set_yticks([])\n",
        "            img_idx += 1\n",
        "\n",
        "show_reranking(np_images, scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5628e653-644a-418a-b554-203bea41d0a2",
      "metadata": {
        "id": "5628e653-644a-418a-b554-203bea41d0a2"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "txt = \"this bird has wings that are brown with a white belly\"\n",
        "img_path = \"images/Yellow_Headed_Blackbird_0013_8362.jpg\"\n",
        "\n",
        "img = Image.open(img_path)\n",
        "tf = transforms.Compose([\n",
        "    transforms.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),\n",
        "    transforms.RandomResizedCrop(256, scale=(0.6, 1.0), ratio=(1.0, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "img = tf(img).cuda()\n",
        "\n",
        "sot_token = vocab.encode(\"<|startoftext|>\").ids[0]\n",
        "eot_token = vocab.encode(\"<|endoftext|>\").ids[0]\n",
        "codes = [0] * dalle_dict['hparams']['text_seq_len']\n",
        "text_token = vocab.encode(txt).ids\n",
        "tokens = [sot_token] + text_token + [eot_token]\n",
        "codes[:len(tokens)] = tokens\n",
        "caption_token = torch.LongTensor(codes).cuda()\n",
        "\n",
        "imgs = img.repeat(16,1,1,1)\n",
        "caps = caption_token.repeat(16,1)\n",
        "\n",
        "mask = (caps != 0).cuda()\n",
        "\n",
        "images = dalle.generate_images(\n",
        "        caps,\n",
        "        mask = mask,\n",
        "        img = imgs,\n",
        "        num_init_img_tokens = (100),  # you can set the size of the initial crop, defaults to a little less than ~1/2 of the tokens, as done in the paper\n",
        "        filter_thres = 0.9,\n",
        "        temperature = 1.0\n",
        ")\n",
        "\n",
        "grid = make_grid(images, nrow=4, normalize=False, range=(-1, 1)).cpu()\n",
        "show(grid)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "F2xJouLI7aU2"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b338a8878e74c8e8a75666331231934": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d36c9604ca242df8f7c4351e83a196e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_caaab135490c4e1a9de134d18287d120",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a41fa6a659e84987a3fda99398ef5563",
            "value": 10
          }
        },
        "42c516d1f9554aa480881d309dfbe8da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e60cd7f49ee343e69433abf14c17d772",
            "placeholder": "​",
            "style": "IPY_MODEL_ce482436024942e492651ec508bdb7f0",
            "value": "100%"
          }
        },
        "45f8f979147c4a2d97db0310e34d9ec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a41fa6a659e84987a3fda99398ef5563": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "caaab135490c4e1a9de134d18287d120": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce482436024942e492651ec508bdb7f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e42c33fef9c948cf9657f4ba53000e4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42c516d1f9554aa480881d309dfbe8da",
              "IPY_MODEL_1d36c9604ca242df8f7c4351e83a196e",
              "IPY_MODEL_f1e44170012c4a5a8c4528ac8d9693dd"
            ],
            "layout": "IPY_MODEL_1b338a8878e74c8e8a75666331231934"
          }
        },
        "e60cd7f49ee343e69433abf14c17d772": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1e44170012c4a5a8c4528ac8d9693dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f60e5268d4614f4fb959c98a9f031ff2",
            "placeholder": "​",
            "style": "IPY_MODEL_45f8f979147c4a2d97db0310e34d9ec0",
            "value": " 10/10 [05:16&lt;00:00,  8.62s/it]"
          }
        },
        "f60e5268d4614f4fb959c98a9f031ff2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
